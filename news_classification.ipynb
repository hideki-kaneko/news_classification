{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニュース記事分類\n",
    "\n",
    "ライブドアニュースコーパス（ https://www.rondhuit.com/download.html )を分類します。同コーパスをダウンロードして解凍すると出てくるtextフォルダがこのノートと同じディレクトリにあると仮定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger(\"-Ochasen -d /usr/lib/mecab/dic/mecab-ipadic-neologd/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込みと形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:01<00:00,  6.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# コーパスのパス\n",
    "path = \"text/\"\n",
    "\n",
    "dirs = [i for i in os.listdir(path) if os.path.isdir(os.path.join(path, i))]\n",
    "original = []\n",
    "corpus = []\n",
    "labels = []\n",
    "label_to_class = {i:j for i,j in enumerate(dirs)}\n",
    "for i, d in enumerate(tqdm(dirs)):\n",
    "    p = os.path.join(path, d)\n",
    "    files = [i for i in os.listdir(p) if i[-4:]==\".txt\" and i!=\"LICENSE.txt\"]\n",
    "    for f in files:\n",
    "        with open(os.path.join(p, f), encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()[2:]\n",
    "            lines = \"\".join(lines).replace(\"\\n\",\"\")\n",
    "            original.append(lines)\n",
    "            node = mecab.parseToNode(lines)\n",
    "            wakati = []\n",
    "            while node:\n",
    "                f = node.feature.split(\",\")[0]\n",
    "                if f in [\"名詞\"]: # 今回は名詞のみ抽出した\n",
    "                    wakati.append(node.surface)\n",
    "                node = node.next\n",
    "            wakati = \" \".join(wakati)\n",
    "            wakati\n",
    "            corpus.append(wakati)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, max_features=3000) # とりあえず3000次元とした\n",
    "vector = vectorizer.fit_transform(corpus)\n",
    "vector = vector.toarray()\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7367, 100)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=100) # とりあえず100次元とした\n",
    "red_vector = pca.fit_transform(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グリッドサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 500}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9421745622370028"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'C':[100, 200, 300, 400, 500, 600] #80\n",
    "}\n",
    "clf = GridSearchCV(svm.SVC(), parameters, n_jobs=-1)\n",
    "clf.fit(red_vector, labels)\n",
    "print(clf.best_params_)\n",
    "\n",
    "pred = clf.predict(red_vector)\n",
    "accuracy_score(labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クロスバリデーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精度: 0.8809429403686003 (+- 0.038053964794232266 )\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, red_vector, labels, cv=5)\n",
    "print(\"精度:\", scores.mean(), \"(+-\", scores.std(), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各カテゴリの代表文章抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カテゴリ:  dokujo-tsushin\n",
      "典型的な文章（特徴: ガール,ネット,全員,友達,チェック )\n",
      "【元気東北・岩手編】河童ってホントは何者なのか　子供のように小柄で、頭頂部がハゲており、口がくちばしのように尖っている。相撲とキュウリが大好きで、体臭が魚のようにキツい……と言えば、誰の顔を思い浮かべ …\n",
      "---------------\n",
      "カテゴリ:  it-life-hack\n",
      "典型的な文章（特徴: 機能,レビュー,リモコン,ホーム,設定 )\n",
      "三池崇史監督、『一命』に続き『愛と誠』で2年連続カンヌ選出　原作・梶原一騎、作画・ながやす巧の名作コミックを、30年余の時を経て三池崇史監督が映画化。妻夫木聡演じる超不良・太賀誠に、ヒロイン武井咲演じ …\n",
      "---------------\n",
      "カテゴリ:  kaden-channel\n",
      "典型的な文章（特徴: hd,制作,映像,レンジ,所有 )\n",
      "究極のヴァンパイア・ハンター、“戦う聖職者”の聖書に隠された武器　1892年に出版された、アイルランド人作家ブラム・ストーカーの小説「ドラキュラ」に登場する男性吸血鬼をはじめ、2012年を迎えた現在も …\n",
      "---------------\n",
      "カテゴリ:  livedoor-homme\n",
      "典型的な文章（特徴: スマホ,ストア,1月,mbps,for )\n",
      "『ターミネーター』と『チャーリーズ・エンジェル』を掛け合わせた面白さ　CIA（アメリカ中央情報局）、米大統領の直轄組織であり、外交・国防政策の決定に必要な諜報・謀略活動を行う。そのエージェントと言った …\n",
      "---------------\n",
      "カテゴリ:  movie-enter\n",
      "典型的な文章（特徴: 映画,公開,決定,貞子,アベンジャーズ )\n",
      "【週末映画まとめ読み】注目するのは「貞子の野球カード」か「ボーイッシュ美少女の恋愛トーク」か　今週1週間に起こった出来事を振り返る「週末映画まとめ読み」。先週末の動員ランキングは『メン・イン・ブラック …\n",
      "---------------\n",
      "カテゴリ:  peachy\n",
      "典型的な文章（特徴: 旅行,アート,アイテム,バッグ,便利 )\n",
      "【いちおう妖ヶ劇場】第10話：超能力でスカートをすかっとめくるの巻　元お嬢様から貧乏に転落した勅使河原 栄華（てしがわら えいか）は、映画好きのネコ仙人ら妖怪たちが住むアパートで、貧しいながらも楽しく …\n",
      "---------------\n",
      "カテゴリ:  smax\n",
      "典型的な文章（特徴: optimus,搭載,05,端子,it )\n",
      "アントニオ・バンデラスが「スマイルを届ける」と日本にメッセージ　「本当に興奮しているよ！僕たちは、ほぼ2年半かけてこの作品に取り組んできたんだ。」アントニオ・バンデラスが力強く語る。　23日、ロサンゼ …\n",
      "---------------\n",
      "カテゴリ:  sports-watch\n",
      "典型的な文章（特徴: さん,自分,写真ギャラリー,スタジオ,トーク )\n",
      "最強のハンマー×盾、『アベンジャーズ』本編映像を先行公開　いよいよ、8月14日の日本公開までひと月を切った最強映画『アベンジャーズ』。『アバター』『タイタニック』に迫る第3位の全世界歴代興行収入を記録 …\n",
      "---------------\n",
      "カテゴリ:  topic-news\n",
      "典型的な文章（特徴: akb,応援,本当,事実,さん )\n",
      "【週末映画まとめ読み】注目の若手女優、セクシーグラドル、天才空手美少女を独り占め　今週1週間に起こった出来事を振り返る「週末映画まとめ読み」。ゴールデン・ウィークも終わりを迎えた先週末の動員ランキング …\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "labels_array = np.asarray(labels)\n",
    "original = np.asarray(original)\n",
    "\n",
    "for i, j in label_to_class.items():\n",
    "    print(\"カテゴリ: \", j)\n",
    "    idx = np.argmax(vector[labels_array==i].sum(axis=1))\n",
    "    feature_idx = np.argsort(-vector[labels_array==i][idx])[:5]\n",
    "    print(\"典型的な文章（特徴:\", \",\".join([features[i] for i in feature_idx]), \")\")\n",
    "    print(original[labels_array==4][idx][:100], \"…\")\n",
    "    print(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
